"""
Embedding service using Ollama
"""

import httpx
from typing import List
import logging

logger = logging.getLogger("shield-orchestrator.embeddings")


class EmbeddingService:
    """
    Ollama embedding service.
    
    Uses mxbai-embed-large model (1024 dimensions).
    """
    
    def __init__(self):
        self.ollama_url = "{{ ollama_url }}"
        self.model = "{{ ollama_embedding_model }}"
        self.dimension = {{ ollama_embedding_dim }}
    
    async def get_embedding(self, text: str) -> List[float]:
        """
        Get embedding for single text.
        
        Args:
            text: Text to embed
        
        Returns:
            1024-dimensional embedding vector
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.ollama_url}/api/embeddings",
                json={
                    "model": self.model,
                    "prompt": text
                }
            )
            response.raise_for_status()
            result = response.json()
            return result["embedding"]
    
    async def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Get embeddings for multiple texts (batched).
        
        Args:
            texts: List of texts to embed
        
        Returns:
            List of embedding vectors
        """
        import asyncio
        
        async def _get_single_embedding(client: httpx.AsyncClient, text: str) -> List[float]:
            """Get embedding for a single text"""
            response = await client.post(
                f"{self.ollama_url}/api/embeddings",
                json={
                    "model": self.model,
                    "prompt": text
                }
            )
            response.raise_for_status()
            result = response.json()
            return result["embedding"]
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            # Process all texts concurrently
            embeddings = await asyncio.gather(
                *[_get_single_embedding(client, text) for text in texts]
            )
        
        logger.debug(f"Generated {len(embeddings)} embeddings concurrently")
        return embeddings


# Global service instance
embedding_service = EmbeddingService()


async def check_ollama_health() -> dict:
    """
    Check Ollama health for /health/detailed endpoint.
    """
    import time
    
    try:
        start = time.time()
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{embedding_service.ollama_url}/api/tags")
            response.raise_for_status()
            models = response.json()
        
        latency_ms = (time.time() - start) * 1000
        
        model_names = [m["name"] for m in models.get("models", [])]
        
        return {
            "status": "up",
            "latency_ms": round(latency_ms, 2),
            "models": model_names
        }
    except Exception as e:
        logger.error(f"Ollama health check failed: {str(e)}")
        return {
            "status": "down",
            "error": str(e),
            "latency_ms": 0
        }
