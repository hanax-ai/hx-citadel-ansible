#!/usr/bin/env python3
"""
Shield MCP Server - FastMCP Implementation
Generated by Ansible for {{ ansible_hostname }}

Tools provided:
- crawl_web: Web crawling with Crawl4AI (HTTP 202 async pattern)
- ingest_doc: Document processing with Docling (PDF, DOCX, TXT, MD)
- qdrant_find: Semantic vector search in Qdrant
- qdrant_store: Store text with embeddings in Qdrant
- lightrag_query: Query knowledge base with LightRAG hybrid retrieval
- health_check: Comprehensive health monitoring

Helper functions:
- generate_embedding: Generate embeddings via Ollama
"""

import os
import sys
import asyncio
from pathlib import Path
from typing import Optional, List
from urllib.parse import urlparse

# Add current directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

from fastmcp import FastMCP
from logging_config import configure_structured_logging, get_logger
from enhanced_health_check import comprehensive_health_check

# Third-party imports
import httpx
import pybreaker
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_crawler_strategy import AsyncCrawlerStrategy
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue

# Configure structured logging
logger = configure_structured_logging()

# Initialize FastMCP server
mcp = FastMCP("{{ fastmcp_server_name }}")

# Load environment variables
FASTMCP_PORT = int(os.getenv("FASTMCP_PORT", "{{ fastmcp_port }}"))
QDRANT_URL = os.getenv("QDRANT_URL", "{{ qdrant_url }}")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "{{ qdrant_api_key }}")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "{{ ollama_base_url }}")
ORCHESTRATOR_BASE_URL = os.getenv("ORCHESTRATOR_BASE_URL", "{{ orchestrator_base_url }}")

# Qdrant configuration
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "shield_knowledge_base")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
EMBEDDING_DIMENSION = int(os.getenv("EMBEDDING_DIMENSION", "768"))

# Circuit Breaker configuration
# Protects against cascading failures when orchestrator is down
# Opens after 5 failures, stays open for 60s, half-open allows 1 test request
orchestrator_breaker = pybreaker.CircuitBreaker(
    fail_max=5,              # Open after 5 failures
    timeout_duration=60,     # Stay open for 60 seconds
    name="orchestrator_api",
    listeners=[
        # Log circuit state changes
        lambda state_change: logger.warning(
            "circuit_breaker_state_change",
            circuit="orchestrator_api",
            old_state=state_change.previous_state.name if hasattr(state_change, 'previous_state') else 'unknown',
            new_state=state_change.state.name if hasattr(state_change, 'state') else 'unknown'
        ) if hasattr(state_change, 'state') else None
    ]
)


# Circuit Breaker Wrapper (TASK-014)

async def call_orchestrator_api(
    endpoint: str,
    method: str = "POST",
    json_data: Optional[dict] = None,
    timeout: float = 30.0
) -> dict:
    """
    Call orchestrator API with circuit breaker protection
    
    This wrapper provides fast-fail behavior when the orchestrator is down,
    preventing cascading failures and resource exhaustion.
    
    Args:
        endpoint: API endpoint path (e.g., "/lightrag/ingest-async")
        method: HTTP method (default: POST)
        json_data: JSON payload for POST requests
        timeout: Request timeout in seconds
    
    Returns:
        dict: Response JSON from orchestrator
    
    Raises:
        pybreaker.CircuitBreakerError: If circuit is open (orchestrator unavailable)
        httpx.HTTPStatusError: If orchestrator returns error status
        httpx.TimeoutException: If request times out
    """
    url = f"{ORCHESTRATOR_BASE_URL}{endpoint}"
    
    logger.debug(
        "orchestrator_api_call",
        url=url,
        method=method,
        circuit_state=orchestrator_breaker.current_state.name
    )
    
    @orchestrator_breaker
    async def _make_request():
        async with httpx.AsyncClient(timeout=timeout) as client:
            if method == "POST":
                response = await client.post(url, json=json_data)
            elif method == "GET":
                response = await client.get(url)
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")
            
            response.raise_for_status()
            return response.json()
    
    try:
        result = await _make_request()
        logger.info(
            "orchestrator_api_success",
            endpoint=endpoint,
            method=method
        )
        return result
    
    except pybreaker.CircuitBreakerError as e:
        # Circuit is open - fast-fail without calling orchestrator
        logger.error(
            "circuit_breaker_open",
            endpoint=endpoint,
            circuit_state=orchestrator_breaker.current_state.name,
            failures=orchestrator_breaker.fail_counter
        )
        raise
    
    except httpx.TimeoutException as e:
        logger.error(
            "orchestrator_timeout",
            endpoint=endpoint,
            timeout=timeout
        )
        raise
    
    except httpx.HTTPStatusError as e:
        logger.error(
            "orchestrator_http_error",
            endpoint=endpoint,
            status_code=e.response.status_code,
            error=str(e)
        )
        raise


# Helper Functions (Single Responsibility Principle)

async def generate_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:
    """
    Generate embeddings using Ollama
    
    Args:
        text: Text to embed
        model: Embedding model to use (default: nomic-embed-text)
    
    Returns:
        List[float]: Embedding vector
    
    Raises:
        HTTPException: If Ollama service is unavailable
    """
    logger.info("generate_embedding_start", text_length=len(text), model=model)
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/embeddings",
                json={
                    "model": model,
                    "prompt": text
                }
            )
            
            response.raise_for_status()
            data = response.json()
            
            embedding = data.get("embedding")
            if not embedding:
                raise ValueError("No embedding returned from Ollama")
            
            logger.info(
                "generate_embedding_success",
                model=model,
                embedding_dimension=len(embedding)
            )
            
            return embedding
    
    except httpx.HTTPStatusError as e:
        logger.error(
            "ollama_embedding_error",
            status_code=e.response.status_code,
            error=str(e)
        )
        raise
    
    except Exception as e:
        logger.error("generate_embedding_error", error=str(e), exc_info=True)
        raise


@mcp.tool()
async def crawl_web(
    url: str,
    max_pages: int = 10,
    allowed_domains: Optional[List[str]] = None,
    max_depth: int = 2
) -> dict:
    """
    Crawl a website using Crawl4AI and send to orchestrator for async ingestion
    
    Args:
        url: The starting URL to crawl
        max_pages: Maximum number of pages to crawl (default: 10)
        allowed_domains: List of allowed domains to crawl (default: same domain as URL)
        max_depth: Maximum crawl depth (default: 2)
    
    Returns:
        dict: HTTP 202-style response with job_id for status tracking
    
    Raises:
        HTTPException: For HTTP errors (403, 404, timeouts)
    """
    logger.info(
        "crawl_web_start",
        url=url,
        max_pages=max_pages,
        allowed_domains=allowed_domains,
        max_depth=max_depth
    )
    
    try:
        # Validate and parse URL (Single Responsibility)
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            error_msg = f"Invalid URL format: {url}"
            logger.error("crawl_web_invalid_url", url=url, error=error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "validation_error"
            }
        
        # Set default allowed_domains to same domain if not provided (Dependency Inversion)
        if allowed_domains is None:
            allowed_domains = [parsed_url.netloc]
        
        # Initialize Crawl4AI async crawler
        async with AsyncWebCrawler(
            verbose=True,
            max_concurrent=3  # Prevent overwhelming target servers
        ) as crawler:
            
            logger.info(
                "crawl4ai_crawling",
                url=url,
                max_pages=max_pages,
                allowed_domains=allowed_domains
            )
            
            # Perform the crawl with error handling
            crawled_pages = []
            pages_crawled = 0
            
            # Start with the initial URL
            urls_to_crawl = [url]
            crawled_urls = set()
            
            while urls_to_crawl and pages_crawled < max_pages:
                current_url = urls_to_crawl.pop(0)
                
                if current_url in crawled_urls:
                    continue
                
                try:
                    # Crawl the page with timeout
                    result = await asyncio.wait_for(
                        crawler.arun(
                            url=current_url,
                            bypass_cache=True
                        ),
                        timeout=30.0  # 30 second timeout per page
                    )
                    
                    if result.success:
                        crawled_pages.append({
                            "url": current_url,
                            "content": result.markdown,
                            "html": result.html,
                            "links": result.links.get("internal", []) if result.links else [],
                            "metadata": {
                                "title": getattr(result, 'title', ''),
                                "status_code": getattr(result, 'status_code', 200)
                            }
                        })
                        crawled_urls.add(current_url)
                        pages_crawled += 1
                        
                        # Add internal links to queue if within allowed domains
                        if result.links and "internal" in result.links:
                            for link in result.links["internal"]:
                                link_domain = urlparse(link).netloc
                                if link_domain in allowed_domains and link not in crawled_urls:
                                    urls_to_crawl.append(link)
                    
                    else:
                        logger.warning(
                            "crawl4ai_page_failed",
                            url=current_url,
                            error=result.error_message if hasattr(result, 'error_message') else "Unknown error"
                        )
                
                except asyncio.TimeoutError:
                    logger.warning("crawl4ai_timeout", url=current_url, timeout=30)
                    # Continue with next URL on timeout
                    continue
                
                except httpx.HTTPStatusError as e:
                    if e.response.status_code in [403, 404]:
                        logger.warning(
                            "crawl4ai_http_error",
                            url=current_url,
                            status_code=e.response.status_code,
                            error=str(e)
                        )
                        # Continue with next URL on 403/404
                        continue
                    else:
                        # Re-raise for other HTTP errors
                        raise
                
                except Exception as page_error:
                    logger.error(
                        "crawl4ai_page_error",
                        url=current_url,
                        error=str(page_error),
                        exc_info=True
                    )
                    # Continue with next URL on error
                    continue
            
            logger.info(
                "crawl4ai_complete",
                url=url,
                pages_crawled=pages_crawled,
                total_content_size=sum(len(p["content"]) for p in crawled_pages)
            )
            
            # Send to orchestrator for async ingestion (HTTP 202 pattern)
            # Using circuit breaker wrapper for resilience
            try:
                ingest_data = await call_orchestrator_api(
                    endpoint="/lightrag/ingest-async",
                    method="POST",
                    json_data={
                        "source_type": "web_crawl",
                        "source_url": url,
                        "content": crawled_pages,
                        "metadata": {
                            "max_pages": max_pages,
                            "allowed_domains": allowed_domains,
                            "max_depth": max_depth,
                            "pages_crawled": pages_crawled
                        }
                    },
                    timeout=30.0
                )
                
                # Return HTTP 202-style response with job_id
                logger.info(
                    "crawl_web_success",
                    url=url,
                    pages_crawled=pages_crawled,
                    job_id=ingest_data.get("job_id")
                )
                
                return {
                    "status": "accepted",  # HTTP 202 Accepted
                    "message": f"Web crawl initiated for {url}",
                    "job_id": ingest_data.get("job_id"),
                    "pages_crawled": pages_crawled,
                    "source_url": url,
                    "check_status_endpoint": f"/jobs/{ingest_data.get('job_id')}"
                }
            
            except pybreaker.CircuitBreakerError:
                # Circuit is open - orchestrator unavailable
                return {
                    "status": "error",
                    "error": "Orchestrator temporarily unavailable (circuit breaker open)",
                    "pages_crawled": pages_crawled,
                    "retry_after": 60
                }
            
            except (httpx.HTTPStatusError, httpx.TimeoutException) as e:
                logger.error(
                    "orchestrator_ingest_error",
                    url=url,
                    error=str(e)
                )
                return {
                    "status": "error",
                    "error": f"Orchestrator ingestion failed: {str(e)}",
                    "pages_crawled": pages_crawled
                }
    
    except httpx.HTTPStatusError as e:
        logger.error(
            "crawl_web_http_error",
            url=url,
            status_code=e.response.status_code,
            error=str(e)
        )
        return {
            "status": "error",
            "error": f"HTTP {e.response.status_code}: {str(e)}",
            "error_type": "http_error",
            "status_code": e.response.status_code
        }
    
    except Exception as e:
        logger.error("crawl_web_error", url=url, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def ingest_doc(
    file_path: str,
    source_name: Optional[str] = None
) -> dict:
    """
    Process and ingest a document using Docling for async ingestion into LightRAG
    
    Args:
        file_path: Path to the document file (PDF, DOCX, TXT, Markdown)
        source_name: Optional name for the document source (defaults to filename)
    
    Returns:
        dict: HTTP 202-style response with job_id for status tracking
    
    Supports:
        - PDF (.pdf)
        - Microsoft Word (.docx, .doc)
        - Plain Text (.txt)
        - Markdown (.md)
    """
    logger.info("ingest_doc_start", file_path=file_path, source_name=source_name)
    
    try:
        # Validate file exists (Single Responsibility)
        file_obj = Path(file_path)
        if not file_obj.exists():
            error_msg = f"File not found: {file_path}"
            logger.error("ingest_doc_file_not_found", file_path=file_path, error=error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "file_not_found"
            }
        
        # Validate file is readable
        if not file_obj.is_file():
            error_msg = f"Path is not a file: {file_path}"
            logger.error("ingest_doc_not_a_file", file_path=file_path, error=error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "invalid_path"
            }
        
        # Detect and validate file format (Dependency Inversion)
        file_extension = file_obj.suffix.lower()
        supported_formats = {
            '.pdf': InputFormat.PDF,
            '.docx': InputFormat.DOCX,
            '.doc': InputFormat.DOCX,  # Docling handles both
            '.txt': InputFormat.MARKDOWN,  # Treat as plain text
            '.md': InputFormat.MARKDOWN
        }
        
        if file_extension not in supported_formats:
            error_msg = f"Unsupported file format: {file_extension}. Supported: {', '.join(supported_formats.keys())}"
            logger.error("ingest_doc_unsupported_format", file_path=file_path, extension=file_extension)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "unsupported_format",
                "supported_formats": list(supported_formats.keys())
            }
        
        input_format = supported_formats[file_extension]
        
        # Set default source name
        if source_name is None:
            source_name = file_obj.name
        
        logger.info(
            "docling_processing",
            file_path=file_path,
            format=file_extension,
            size_bytes=file_obj.stat().st_size
        )
        
        # Process document with Docling (Open/Closed Principle)
        try:
            # Initialize Docling converter
            converter = DocumentConverter()
            
            # Convert the document
            result = converter.convert(
                source=str(file_obj),
                raises_on_error=False  # Graceful error handling
            )
            
            if not result.document:
                error_msg = "Document conversion failed: No document object returned"
                logger.error("docling_conversion_failed", file_path=file_path)
                return {
                    "status": "error",
                    "error": error_msg,
                    "error_type": "conversion_failed"
                }
            
            # Extract content and metadata
            document = result.document
            content_text = document.export_to_markdown()  # Unified format
            
            # Extract metadata
            metadata = {
                "file_name": file_obj.name,
                "file_path": str(file_obj.absolute()),
                "file_size_bytes": file_obj.stat().st_size,
                "file_format": file_extension,
                "page_count": getattr(document, 'page_count', 0),
                "title": getattr(document, 'title', file_obj.stem),
                "source_name": source_name
            }
            
            logger.info(
                "docling_success",
                file_path=file_path,
                content_length=len(content_text),
                page_count=metadata.get("page_count", 0)
            )
        
        except ValueError as e:
            # Corrupted file or invalid content
            error_msg = f"File appears to be corrupted or invalid: {str(e)}"
            logger.error("docling_corrupted_file", file_path=file_path, error=str(e))
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "corrupted_file"
            }
        
        except Exception as e:
            # Other processing errors
            logger.error("docling_processing_error", file_path=file_path, error=str(e), exc_info=True)
            return {
                "status": "error",
                "error": f"Document processing failed: {str(e)}",
                "error_type": "processing_error"
            }
        
        # Send to orchestrator for async ingestion (HTTP 202 pattern)
        # Using circuit breaker wrapper for resilience
        try:
            ingest_data = await call_orchestrator_api(
                endpoint="/lightrag/ingest-async",
                method="POST",
                json_data={
                    "source_type": "document",
                    "source_name": source_name,
                    "content": content_text,
                    "metadata": metadata
                },
                timeout=30.0
            )
            
            # Return HTTP 202-style response with job_id
            logger.info(
                "ingest_doc_success",
                file_path=file_path,
                content_length=len(content_text),
                job_id=ingest_data.get("job_id")
            )
            
            return {
                "status": "accepted",  # HTTP 202 Accepted
                "message": f"Document ingestion initiated for {source_name}",
                "job_id": ingest_data.get("job_id"),
                "source_name": source_name,
                "file_format": file_extension,
                "content_length": len(content_text),
                "page_count": metadata.get("page_count", 0),
                "check_status_endpoint": f"/jobs/{ingest_data.get('job_id')}"
            }
        
        except pybreaker.CircuitBreakerError:
            # Circuit is open - orchestrator unavailable
            return {
                "status": "error",
                "error": "Orchestrator temporarily unavailable (circuit breaker open)",
                "retry_after": 60
            }
        
        except (httpx.HTTPStatusError, httpx.TimeoutException) as e:
            logger.error(
                "orchestrator_ingest_error",
                file_path=file_path,
                error=str(e)
            )
            return {
                "status": "error",
                "error": f"Orchestrator ingestion failed: {str(e)}",
                "error_type": "orchestrator_error"
            }
    
    except Exception as e:
        logger.error("ingest_doc_error", file_path=file_path, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def qdrant_find(
    query: str,
    collection: Optional[str] = None,
    limit: int = 10,
    score_threshold: float = 0.0,
    filter_conditions: Optional[dict] = None
) -> dict:
    """
    Search vectors in Qdrant using semantic similarity
    
    Args:
        query: Search query text
        collection: Qdrant collection name (default: shield_knowledge_base)
        limit: Number of results to return (default: 10)
        score_threshold: Minimum similarity score (0.0 to 1.0, default: 0.0)
        filter_conditions: Optional filters (e.g., {"field": "value"})
    
    Returns:
        dict: Search results with scores and metadata
    """
    # Set default collection (Dependency Inversion)
    if collection is None:
        collection = QDRANT_COLLECTION
    
    logger.info(
        "qdrant_find_start",
        query=query,
        collection=collection,
        limit=limit,
        score_threshold=score_threshold
    )
    
    try:
        # Generate embedding for query (Single Responsibility)
        try:
            query_embedding = await generate_embedding(query)
        except Exception as e:
            logger.error("embedding_generation_failed", query=query, error=str(e))
            return {
                "status": "error",
                "error": f"Failed to generate embedding: {str(e)}",
                "error_type": "embedding_error"
            }
        
        # Initialize Qdrant client
        async with AsyncQdrantClient(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY if QDRANT_API_KEY else None,
            timeout=30.0
        ) as client:
            
            # Build filter if provided (Open/Closed Principle)
            search_filter = None
            if filter_conditions:
                conditions = []
                for field, value in filter_conditions.items():
                    conditions.append(
                        FieldCondition(
                            key=field,
                            match=MatchValue(value=value)
                        )
                    )
                search_filter = Filter(must=conditions) if conditions else None
            
            logger.info(
                "qdrant_searching",
                collection=collection,
                query_embedding_dim=len(query_embedding),
                filter_applied=filter_conditions is not None
            )
            
            # Perform vector search
            try:
                search_result = await client.search(
                    collection_name=collection,
                    query_vector=query_embedding,
                    limit=limit,
                    score_threshold=score_threshold,
                    query_filter=search_filter
                )
            
            except Exception as e:
                error_msg = str(e)
                if "not found" in error_msg.lower():
                    logger.error("qdrant_collection_not_found", collection=collection)
                    return {
                        "status": "error",
                        "error": f"Collection '{collection}' not found",
                        "error_type": "collection_not_found"
                    }
                else:
                    raise
            
            # Format results (Interface Segregation)
            results = []
            for scored_point in search_result:
                result_entry = {
                    "id": str(scored_point.id),
                    "score": float(scored_point.score),
                    "payload": scored_point.payload or {}
                }
                results.append(result_entry)
            
            logger.info(
                "qdrant_find_success",
                query=query,
                collection=collection,
                result_count=len(results),
                top_score=results[0]["score"] if results else 0.0
            )
            
            return {
                "status": "success",
            "query": query,
            "collection": collection,
                "result_count": len(results),
                "results": results,
                "score_threshold": score_threshold,
                "embedding_model": EMBEDDING_MODEL
            }
    
    except httpx.HTTPStatusError as e:
        logger.error(
            "qdrant_http_error",
            query=query,
            status_code=e.response.status_code,
            error=str(e)
        )
        return {
            "status": "error",
            "error": f"Qdrant HTTP error: {e.response.status_code}",
            "error_type": "http_error"
        }
    
    except Exception as e:
        logger.error("qdrant_find_error", query=query, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def qdrant_store(
    text: str,
    metadata: Optional[dict] = None,
    collection: Optional[str] = None,
    point_id: Optional[str] = None
) -> dict:
    """
    Store text with embeddings in Qdrant
    
    Args:
        text: Text to embed and store
        metadata: Optional metadata dictionary to store with the vector
        collection: Qdrant collection name (default: shield_knowledge_base)
        point_id: Optional specific ID for the point (auto-generated if None)
    
    Returns:
        dict: Storage confirmation with point ID
    """
    # Set default collection (Dependency Inversion)
    if collection is None:
        collection = QDRANT_COLLECTION
    
    logger.info(
        "qdrant_store_start",
        text_length=len(text),
        collection=collection,
        has_metadata=metadata is not None,
        has_custom_id=point_id is not None
    )
    
    try:
        # Generate embedding for text (Single Responsibility)
        try:
            embedding = await generate_embedding(text)
        except Exception as e:
            logger.error("embedding_generation_failed", text_length=len(text), error=str(e))
            return {
                "status": "error",
                "error": f"Failed to generate embedding: {str(e)}",
                "error_type": "embedding_error"
            }
        
        # Prepare payload (Open/Closed Principle)
        payload = {
            "text": text,
            "created_at": asyncio.get_event_loop().time(),
            "embedding_model": EMBEDDING_MODEL
        }
        
        # Merge user-provided metadata
        if metadata:
            payload.update(metadata)
        
        # Generate point ID if not provided
        if point_id is None:
            import uuid
            point_id = str(uuid.uuid4())
        
        # Initialize Qdrant client
        async with AsyncQdrantClient(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY if QDRANT_API_KEY else None,
            timeout=30.0
        ) as client:
            
            logger.info(
                "qdrant_storing",
                collection=collection,
                point_id=point_id,
                embedding_dim=len(embedding),
                payload_keys=list(payload.keys())
            )
            
            # Ensure collection exists (create if needed)
            try:
                await client.get_collection(collection_name=collection)
            except Exception:
                # Collection doesn't exist, create it
                logger.info("qdrant_creating_collection", collection=collection)
                await client.create_collection(
                    collection_name=collection,
                    vectors_config=VectorParams(
                        size=EMBEDDING_DIMENSION,
                        distance=Distance.COSINE
                    )
                )
            
            # Upsert point (create or update)
            try:
                point = PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload=payload
                )
                
                await client.upsert(
                    collection_name=collection,
                    points=[point]
                )
            
            except Exception as e:
                logger.error("qdrant_upsert_error", point_id=point_id, error=str(e))
                raise
            
            logger.info(
                "qdrant_store_success",
                collection=collection,
                point_id=point_id,
                text_length=len(text)
            )
            
            return {
                "status": "success",
                "message": f"Vector stored successfully in collection '{collection}'",
                "point_id": point_id,
                "collection": collection,
                "embedding_dimension": len(embedding),
                "embedding_model": EMBEDDING_MODEL,
                "payload_keys": list(payload.keys())
            }
    
    except httpx.HTTPStatusError as e:
        logger.error(
            "qdrant_http_error",
            collection=collection,
            status_code=e.response.status_code,
            error=str(e)
        )
        return {
            "status": "error",
            "error": f"Qdrant HTTP error: {e.response.status_code}",
            "error_type": "http_error"
        }
    
    except Exception as e:
        logger.error("qdrant_store_error", collection=collection, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def lightrag_query(
    query: str,
    mode: str = "hybrid",
    only_need_context: bool = False
) -> dict:
    """
    Query knowledge base using LightRAG hybrid retrieval
    
    Args:
        query: The query text
        mode: Retrieval mode - 'naive', 'local', 'global', or 'hybrid' (default)
        only_need_context: If True, return only context without generating response
    
    Returns:
        dict: Query results with context and optional generated response
    
    Modes:
        - naive: Simple vector search
        - local: Local entity-based retrieval
        - global: Global community-based retrieval
        - hybrid: Combined approach (recommended)
    """
    logger.info(
        "lightrag_query_start",
        query=query,
        mode=mode,
        only_need_context=only_need_context
    )
    
    try:
        # Validate mode (Single Responsibility)
        valid_modes = ["naive", "local", "global", "hybrid"]
        if mode not in valid_modes:
            error_msg = f"Invalid mode '{mode}'. Must be one of: {', '.join(valid_modes)}"
            logger.error("lightrag_invalid_mode", mode=mode, valid_modes=valid_modes)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "validation_error",
                "valid_modes": valid_modes
            }
        
        # Forward to orchestrator LightRAG endpoint (Dependency Inversion)
        # Using circuit breaker wrapper for resilience
        logger.info(
            "lightrag_forwarding",
            orchestrator_url=ORCHESTRATOR_BASE_URL,
            query_length=len(query),
            mode=mode
        )
        
        try:
            result_data = await call_orchestrator_api(
                endpoint="/lightrag/query",
                method="POST",
                json_data={
                    "query": query,
                    "mode": mode,
                    "only_need_context": only_need_context
                },
                timeout=60.0
            )
            
            logger.info(
                "lightrag_query_success",
                query=query,
                mode=mode,
                has_response=not only_need_context,
                context_count=len(result_data.get("context", []))
            )
            
            # Return orchestrator response (Interface Segregation)
            return {
                "status": "success",
                "query": query,
                "mode": mode,
                "response": result_data.get("response"),
                "context": result_data.get("context", []),
                "metadata": result_data.get("metadata", {})
            }
        
        except pybreaker.CircuitBreakerError:
            # Circuit is open - orchestrator unavailable
            logger.error(
                "circuit_breaker_open",
                query=query,
                circuit_state=orchestrator_breaker.current_state.name
            )
            return {
                "status": "error",
                "error": "Orchestrator temporarily unavailable (circuit breaker open)",
                "retry_after": 60,
                "error_type": "circuit_breaker_error"
            }
        
        except (httpx.HTTPStatusError, httpx.TimeoutException) as e:
            logger.error(
                "orchestrator_query_error",
                query=query,
                error=str(e)
            )
            return {
                "status": "error",
                "error": f"Orchestrator query failed: {str(e)}",
                "error_type": "orchestrator_error"
            }
    
    except Exception as e:
        logger.error("lightrag_query_error", query=query, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def health_check() -> dict:
    """
    Comprehensive health check of MCP server and dependencies
    
    Returns:
        dict: Health status of server and all dependencies, including circuit breaker state
    """
    logger.info("health_check_requested")
    
    try:
        health_status = await comprehensive_health_check()
        
        # Add circuit breaker state metrics (TASK-016)
        health_status["circuit_breakers"] = {
            "orchestrator": {
                "state": orchestrator_breaker.current_state.name,
                "fail_counter": orchestrator_breaker.fail_counter,
                "fail_max": orchestrator_breaker.fail_max,
                "timeout_duration": orchestrator_breaker.timeout_duration,
                "last_failure": str(orchestrator_breaker.last_failure_time) if orchestrator_breaker.last_failure_time else None,
                "excluded_exceptions": [e.__name__ for e in orchestrator_breaker.excluded_exceptions] if orchestrator_breaker.excluded_exceptions else []
            }
        }
        
        logger.info(
            "health_check_complete",
            status=health_status.get("overall_status"),
            circuit_state=orchestrator_breaker.current_state.name
        )
        return health_status
    
    except Exception as e:
        logger.error("health_check_error", error=str(e), exc_info=True)
        return {
            "status": "unhealthy",
            "error": str(e)
        }


def main():
    """Main entry point"""
    logger.info(
        "server_starting",
        server_name="{{ fastmcp_server_name }}",
        port=FASTMCP_PORT,
        qdrant_url=QDRANT_URL,
        ollama_url=OLLAMA_BASE_URL,
        orchestrator_url=ORCHESTRATOR_BASE_URL
    )
    
    try:
        # Run the FastMCP server
        # CRITICAL: Bind to 0.0.0.0 to accept connections from all network interfaces
        mcp.run(transport="sse", host="0.0.0.0", port=FASTMCP_PORT)
    
    except KeyboardInterrupt:
        logger.info("server_shutdown_requested")
    
    except Exception as e:
        logger.error("server_error", error=str(e), exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
