#!/usr/bin/env python3
"""
Shield MCP Server - FastMCP Implementation
Generated by Ansible for {{ ansible_hostname }}

Tools provided:
- crawl_web: Web crawling with Crawl4AI
- process_document: Document processing with Docling
- rag_query: RAG queries with LightRAG
- vector_search: Qdrant vector search
- health_check: Enhanced health monitoring
"""

import os
import sys
import asyncio
from pathlib import Path
from typing import Optional, List
from urllib.parse import urlparse

# Add current directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

from fastmcp import FastMCP
from logging_config import configure_structured_logging, get_logger
from enhanced_health_check import comprehensive_health_check

# Third-party imports
import httpx
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_crawler_strategy import AsyncCrawlerStrategy
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat

# Configure structured logging
logger = configure_structured_logging()

# Initialize FastMCP server
mcp = FastMCP("{{ fastmcp_server_name }}")

# Load environment variables
FASTMCP_PORT = int(os.getenv("FASTMCP_PORT", "{{ fastmcp_port }}"))
QDRANT_URL = os.getenv("QDRANT_URL", "{{ qdrant_url }}")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "{{ qdrant_api_key }}")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "{{ ollama_base_url }}")
ORCHESTRATOR_BASE_URL = os.getenv("ORCHESTRATOR_BASE_URL", "{{ orchestrator_base_url }}")


@mcp.tool()
async def crawl_web(
    url: str,
    max_pages: int = 10,
    allowed_domains: Optional[List[str]] = None,
    max_depth: int = 2
) -> dict:
    """
    Crawl a website using Crawl4AI and send to orchestrator for async ingestion
    
    Args:
        url: The starting URL to crawl
        max_pages: Maximum number of pages to crawl (default: 10)
        allowed_domains: List of allowed domains to crawl (default: same domain as URL)
        max_depth: Maximum crawl depth (default: 2)
    
    Returns:
        dict: HTTP 202-style response with job_id for status tracking
    
    Raises:
        HTTPException: For HTTP errors (403, 404, timeouts)
    """
    logger.info(
        "crawl_web_start",
        url=url,
        max_pages=max_pages,
        allowed_domains=allowed_domains,
        max_depth=max_depth
    )
    
    try:
        # Validate and parse URL (Single Responsibility)
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            error_msg = f"Invalid URL format: {url}"
            logger.error("crawl_web_invalid_url", url=url, error=error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "validation_error"
            }
        
        # Set default allowed_domains to same domain if not provided (Dependency Inversion)
        if allowed_domains is None:
            allowed_domains = [parsed_url.netloc]
        
        # Initialize Crawl4AI async crawler
        async with AsyncWebCrawler(
            verbose=True,
            max_concurrent=3  # Prevent overwhelming target servers
        ) as crawler:
            
            logger.info(
                "crawl4ai_crawling",
                url=url,
                max_pages=max_pages,
                allowed_domains=allowed_domains
            )
            
            # Perform the crawl with error handling
            crawled_pages = []
            pages_crawled = 0
            
            # Start with the initial URL
            urls_to_crawl = [url]
            crawled_urls = set()
            
            while urls_to_crawl and pages_crawled < max_pages:
                current_url = urls_to_crawl.pop(0)
                
                if current_url in crawled_urls:
                    continue
                
                try:
                    # Crawl the page with timeout
                    result = await asyncio.wait_for(
                        crawler.arun(
                            url=current_url,
                            bypass_cache=True
                        ),
                        timeout=30.0  # 30 second timeout per page
                    )
                    
                    if result.success:
                        crawled_pages.append({
                            "url": current_url,
                            "content": result.markdown,
                            "html": result.html,
                            "links": result.links.get("internal", []) if result.links else [],
                            "metadata": {
                                "title": getattr(result, 'title', ''),
                                "status_code": getattr(result, 'status_code', 200)
                            }
                        })
                        crawled_urls.add(current_url)
                        pages_crawled += 1
                        
                        # Add internal links to queue if within allowed domains
                        if result.links and "internal" in result.links:
                            for link in result.links["internal"]:
                                link_domain = urlparse(link).netloc
                                if link_domain in allowed_domains and link not in crawled_urls:
                                    urls_to_crawl.append(link)
                    
                    else:
                        logger.warning(
                            "crawl4ai_page_failed",
                            url=current_url,
                            error=result.error_message if hasattr(result, 'error_message') else "Unknown error"
                        )
                
                except asyncio.TimeoutError:
                    logger.warning("crawl4ai_timeout", url=current_url, timeout=30)
                    # Continue with next URL on timeout
                    continue
                
                except httpx.HTTPStatusError as e:
                    if e.response.status_code in [403, 404]:
                        logger.warning(
                            "crawl4ai_http_error",
                            url=current_url,
                            status_code=e.response.status_code,
                            error=str(e)
                        )
                        # Continue with next URL on 403/404
                        continue
                    else:
                        # Re-raise for other HTTP errors
                        raise
                
                except Exception as page_error:
                    logger.error(
                        "crawl4ai_page_error",
                        url=current_url,
                        error=str(page_error),
                        exc_info=True
                    )
                    # Continue with next URL on error
                    continue
            
            logger.info(
                "crawl4ai_complete",
                url=url,
                pages_crawled=pages_crawled,
                total_content_size=sum(len(p["content"]) for p in crawled_pages)
            )
            
            # Send to orchestrator for async ingestion (HTTP 202 pattern)
            async with httpx.AsyncClient(timeout=30.0) as client:
                try:
                    ingest_response = await client.post(
                        f"{ORCHESTRATOR_BASE_URL}/lightrag/ingest-async",
                        json={
                            "source_type": "web_crawl",
                            "source_url": url,
                            "content": crawled_pages,
                            "metadata": {
                                "max_pages": max_pages,
                                "allowed_domains": allowed_domains,
                                "max_depth": max_depth,
                                "pages_crawled": pages_crawled
                            }
                        }
                    )
                    
                    ingest_response.raise_for_status()
                    ingest_data = ingest_response.json()
                    
                    # Return HTTP 202-style response with job_id
                    logger.info(
                        "crawl_web_success",
                        url=url,
                        pages_crawled=pages_crawled,
                        job_id=ingest_data.get("job_id")
                    )
                    
                    return {
                        "status": "accepted",  # HTTP 202 Accepted
                        "message": f"Web crawl initiated for {url}",
                        "job_id": ingest_data.get("job_id"),
                        "pages_crawled": pages_crawled,
                        "source_url": url,
                        "check_status_endpoint": f"/jobs/{ingest_data.get('job_id')}"
                    }
                
                except httpx.HTTPStatusError as e:
                    logger.error(
                        "orchestrator_ingest_error",
                        url=url,
                        status_code=e.response.status_code,
                        error=str(e)
                    )
                    return {
                        "status": "error",
                        "error": f"Orchestrator ingestion failed: {e.response.status_code}",
                        "error_type": "orchestrator_error"
                    }
                
                except httpx.TimeoutError:
                    logger.error("orchestrator_timeout", url=url)
                    return {
                        "status": "error",
                        "error": "Orchestrator ingestion timeout",
                        "error_type": "timeout_error"
                    }
    
    except httpx.HTTPStatusError as e:
        logger.error(
            "crawl_web_http_error",
            url=url,
            status_code=e.response.status_code,
            error=str(e)
        )
        return {
            "status": "error",
            "error": f"HTTP {e.response.status_code}: {str(e)}",
            "error_type": "http_error",
            "status_code": e.response.status_code
        }
    
    except Exception as e:
        logger.error("crawl_web_error", url=url, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def ingest_doc(
    file_path: str,
    source_name: Optional[str] = None
) -> dict:
    """
    Process and ingest a document using Docling for async ingestion into LightRAG
    
    Args:
        file_path: Path to the document file (PDF, DOCX, TXT, Markdown)
        source_name: Optional name for the document source (defaults to filename)
    
    Returns:
        dict: HTTP 202-style response with job_id for status tracking
    
    Supports:
        - PDF (.pdf)
        - Microsoft Word (.docx, .doc)
        - Plain Text (.txt)
        - Markdown (.md)
    """
    logger.info("ingest_doc_start", file_path=file_path, source_name=source_name)
    
    try:
        # Validate file exists (Single Responsibility)
        file_obj = Path(file_path)
        if not file_obj.exists():
            error_msg = f"File not found: {file_path}"
            logger.error("ingest_doc_file_not_found", file_path=file_path, error=error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "file_not_found"
            }
        
        # Validate file is readable
        if not file_obj.is_file():
            error_msg = f"Path is not a file: {file_path}"
            logger.error("ingest_doc_not_a_file", file_path=file_path, error=error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "invalid_path"
            }
        
        # Detect and validate file format (Dependency Inversion)
        file_extension = file_obj.suffix.lower()
        supported_formats = {
            '.pdf': InputFormat.PDF,
            '.docx': InputFormat.DOCX,
            '.doc': InputFormat.DOCX,  # Docling handles both
            '.txt': InputFormat.MARKDOWN,  # Treat as plain text
            '.md': InputFormat.MARKDOWN
        }
        
        if file_extension not in supported_formats:
            error_msg = f"Unsupported file format: {file_extension}. Supported: {', '.join(supported_formats.keys())}"
            logger.error("ingest_doc_unsupported_format", file_path=file_path, extension=file_extension)
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "unsupported_format",
                "supported_formats": list(supported_formats.keys())
            }
        
        input_format = supported_formats[file_extension]
        
        # Set default source name
        if source_name is None:
            source_name = file_obj.name
        
        logger.info(
            "docling_processing",
            file_path=file_path,
            format=file_extension,
            size_bytes=file_obj.stat().st_size
        )
        
        # Process document with Docling (Open/Closed Principle)
        try:
            # Initialize Docling converter
            converter = DocumentConverter()
            
            # Convert the document
            result = converter.convert(
                source=str(file_obj),
                raises_on_error=False  # Graceful error handling
            )
            
            if not result.document:
                error_msg = "Document conversion failed: No document object returned"
                logger.error("docling_conversion_failed", file_path=file_path)
                return {
                    "status": "error",
                    "error": error_msg,
                    "error_type": "conversion_failed"
                }
            
            # Extract content and metadata
            document = result.document
            content_text = document.export_to_markdown()  # Unified format
            
            # Extract metadata
            metadata = {
                "file_name": file_obj.name,
                "file_path": str(file_obj.absolute()),
                "file_size_bytes": file_obj.stat().st_size,
                "file_format": file_extension,
                "page_count": getattr(document, 'page_count', 0),
                "title": getattr(document, 'title', file_obj.stem),
                "source_name": source_name
            }
            
            logger.info(
                "docling_success",
                file_path=file_path,
                content_length=len(content_text),
                page_count=metadata.get("page_count", 0)
            )
        
        except ValueError as e:
            # Corrupted file or invalid content
            error_msg = f"File appears to be corrupted or invalid: {str(e)}"
            logger.error("docling_corrupted_file", file_path=file_path, error=str(e))
            return {
                "status": "error",
                "error": error_msg,
                "error_type": "corrupted_file"
            }
        
        except Exception as e:
            # Other processing errors
            logger.error("docling_processing_error", file_path=file_path, error=str(e), exc_info=True)
            return {
                "status": "error",
                "error": f"Document processing failed: {str(e)}",
                "error_type": "processing_error"
            }
        
        # Send to orchestrator for async ingestion (HTTP 202 pattern)
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                ingest_response = await client.post(
                    f"{ORCHESTRATOR_BASE_URL}/lightrag/ingest-async",
                    json={
                        "source_type": "document",
                        "source_name": source_name,
                        "content": content_text,
                        "metadata": metadata
                    }
                )
                
                ingest_response.raise_for_status()
                ingest_data = ingest_response.json()
                
                # Return HTTP 202-style response with job_id
                logger.info(
                    "ingest_doc_success",
                    file_path=file_path,
                    content_length=len(content_text),
                    job_id=ingest_data.get("job_id")
                )
                
                return {
                    "status": "accepted",  # HTTP 202 Accepted
                    "message": f"Document ingestion initiated for {source_name}",
                    "job_id": ingest_data.get("job_id"),
                    "source_name": source_name,
                    "file_format": file_extension,
                    "content_length": len(content_text),
                    "page_count": metadata.get("page_count", 0),
                    "check_status_endpoint": f"/jobs/{ingest_data.get('job_id')}"
                }
            
            except httpx.HTTPStatusError as e:
                logger.error(
                    "orchestrator_ingest_error",
                    file_path=file_path,
                    status_code=e.response.status_code,
                    error=str(e)
                )
                return {
                    "status": "error",
                    "error": f"Orchestrator ingestion failed: {e.response.status_code}",
                    "error_type": "orchestrator_error"
                }
            
            except httpx.TimeoutError:
                logger.error("orchestrator_timeout", file_path=file_path)
                return {
                    "status": "error",
                    "error": "Orchestrator ingestion timeout",
                    "error_type": "timeout_error"
                }
    
    except Exception as e:
        logger.error("ingest_doc_error", file_path=file_path, error=str(e), exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "error_type": "unknown_error"
        }


@mcp.tool()
async def vector_search(query: str, collection: str = "default", limit: int = 10) -> dict:
    """
    Search vectors in Qdrant
    
    Args:
        query: Search query
        collection: Qdrant collection name
        limit: Number of results to return
    
    Returns:
        dict: Search results with scores
    """
    logger.info("vector_search_start", query=query, collection=collection, limit=limit)
    
    try:
        # TODO: Implement Qdrant integration
        result = {
            "query": query,
            "collection": collection,
            "status": "success",
            "results": []
        }
        
        logger.info("vector_search_success", query=query, result_count=len(result["results"]))
        return result
    
    except Exception as e:
        logger.error("vector_search_error", query=query, error=str(e), exc_info=True)
        return {"status": "error", "error": str(e)}


@mcp.tool()
async def health_check() -> dict:
    """
    Comprehensive health check of MCP server and dependencies
    
    Returns:
        dict: Health status of server and all dependencies
    """
    logger.info("health_check_requested")
    
    try:
        health_status = await comprehensive_health_check()
        logger.info("health_check_complete", status=health_status.get("overall_status"))
        return health_status
    
    except Exception as e:
        logger.error("health_check_error", error=str(e), exc_info=True)
        return {
            "status": "unhealthy",
            "error": str(e)
        }


def main():
    """Main entry point"""
    logger.info(
        "server_starting",
        server_name="{{ fastmcp_server_name }}",
        port=FASTMCP_PORT,
        qdrant_url=QDRANT_URL,
        ollama_url=OLLAMA_BASE_URL,
        orchestrator_url=ORCHESTRATOR_BASE_URL
    )
    
    try:
        # Run the FastMCP server
        # CRITICAL: Bind to 0.0.0.0 to accept connections from all network interfaces
        mcp.run(transport="sse", host="0.0.0.0", port=FASTMCP_PORT)
    
    except KeyboardInterrupt:
        logger.info("server_shutdown_requested")
    
    except Exception as e:
        logger.error("server_error", error=str(e), exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
